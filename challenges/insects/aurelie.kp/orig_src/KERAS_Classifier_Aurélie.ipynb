{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Insects classification\"\n",
    "authors:\n",
    "- Aurélie\n",
    "tags:\n",
    "- RAMP\n",
    "- ConvNets\n",
    "- Challenge\n",
    "created_at: 2016-11-08\n",
    "updated_at: 2016-11-08\n",
    "tldr: \n",
    "    Insect classification challenge\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pollenating Insects classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context - aim\n",
    "The aim is to build an image classification model allowing to predict species of insects from pictures. \n",
    "\n",
    "The dataset consists in 20348 pictures of insects from different species gathered from the SPIPOLL project and labeled. There are 18 different classes each one corresponding to a different insect specie. Each picture is a 64x64 colored image, which makes a total of 64×64×3=12288 features per picture. \n",
    "\n",
    "*Reference* : http://www.spipoll.org/\n",
    "\n",
    "## Image recognition and neural networks\n",
    "\n",
    "Over the past few years, neural nets have proven to be very efficient as regards image classification. As an example, \"basic\" multilayer perceptrons can yield very good results in terms of classification errors on the well-known handwritten digits [MNIST dataset](http://yann.lecun.com/exdb/mnist/) (ref : [D. C. Ciresan et al. (2010)](https://arxiv.org/pdf/1003.0358.pdf))\n",
    "\n",
    "*To get familiar with neural networks (with a nice tutorial on the handwritten digits classification problem) : * http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "\n",
    "Object (or insect...) recognition using \"real life\" images can however prove to be tricky, and this for many reasons. A few are listed below :\n",
    "- The separation between the object and its background is not necessarily obvious\n",
    "- Several pictures of a same object can actually look quite different the one from the other. For example, the object's location in the image or the illumination can vary, which means the classification model needs to be invariant under certain transformations (translational symmetry for example)\n",
    "- Efficient computer vision requires models that are able to exploit the 2D topology of pixels, as well as locality.\n",
    "\n",
    "\n",
    "Because of their particular properties, convolutional neural networks (CNNs) allow to address the issues listed above.\n",
    "\n",
    "\n",
    "### Convolutional neural networks\n",
    "\n",
    "By construction, CNNs are well suited for image classification :\n",
    "- from one convolutional layer (CL) to the next one, only a few units are connected together, which allows local treatment of subsets of pixels\n",
    "- parameter sharing in one given CL contributes to translational invariance of the model\n",
    "- In practice, the two constraints listed above reduce drastically the number of model parameters to be computed, and then allow to train quite complex models in a reasonable time.\n",
    "\n",
    "*Some useful reference to gain knowledge of CNNs : * \n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "\n",
    "A basic CNN consists in successions of convolutional layers (CL) and pooling layers (PL), the latter allowing to reduce the number of parameters to be computed in the network. Those successions of CLs and PLs allow to perform feature extraction. For image classification, the output layer is a fully connected NN layer with a number of units equal to the number of classes. The output layer activation is a softmax, so that the i$^{th}$ output unit activation is consistent with the probability that the image belongs to class i.\n",
    "\n",
    "It's also common to see in a CNN, the CLs and PLs being combined with some rectification (non-linearities) and normalization layers that can drastically improve the classification accuracy ([Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building CNNs with Keras\n",
    "\n",
    "Below are loaded some useful libraries for building, training and evaluating neural nets.\n",
    "- [Keras](https://keras.io/) is a python library running either on [Tensorflow](https://www.tensorflow.org/) or [Theano](http://deeplearning.net/software/theano/). The following pieces of codes are valid for a Tensorflow implementation. [Here are some instructions to install Tensorflow](https://github.com/tensorflow/tensorflow#download-and-setup). As training neural nets can be quite computationally costly, it is recommended to install the gpu version of tensorflow (obviously, it's possible only if you have a dedicated GPU!).\n",
    "\n",
    "\n",
    "- In what's next we'll use some methods that are implemented in the well-known machine learning library [scikit-learn](http://scikit-learn.org/stable/). In particular, the methods cross_val_score() and GridSearchCV() will be used, respectively to apply some unit tests to the models and to perform grid searches on model hyperparameters. For those functions to be called on Keras models, those latter will be wrapped into classes that are \"compatible\" with scikit-learn. [Here is some useful tutorial to build scitkit-learn wrappers for estimators](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Convolution2D, MaxPooling2D, AveragePooling2D, Flatten, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "Remark : 10% of the examples are staged as a test set that will be used to evaluate classification accuracy with the model chosen from hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = np.load('train_64x64.npz')\n",
    "\n",
    "X = f['X']\n",
    "print('X shape : ', X.shape)\n",
    "X_flat = X.reshape((20348,64*64*3))\n",
    "print('X_flat shape : ', X_flat.shape)\n",
    "\n",
    "y = f['y']\n",
    "print(y.shape)\n",
    "# encode class values\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "print('y shape : ', encoded_y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, encoded_y, test_size=0.1, random_state=0, stratify=encoded_y)\n",
    "print('Train set size : ', X_train.shape[0])\n",
    "print('Test set size : ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "The pipeline to perform model selection is inspired from [Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf).\n",
    "\n",
    "### Model architecture\n",
    "\n",
    "In this paper, the impact of the following model properties on object classification accuracy is investigated :\n",
    "- number of convolutional layers (CL) needed to perform feature extraction\n",
    "- type of pooling (PL) used (average pooling vs. max pooling)\n",
    "- role of rectification layers (RL)\n",
    "\n",
    "\n",
    "In the following, those criteria will be tested so as to find the model \"architecture\" that is best suited for our classification problem. This will be done by training different models (with different numbers of CLs, and varied types of PL / RL) and evaluating classification accuracy by cross-validation.\n",
    "\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "Once the model architecture is determined, some hyperparameters tuning is performed by using grid search. The concerned hyperparameters are :\n",
    "- number of feature maps in CLs\n",
    "- dimensions of feature maps in CLs\n",
    "- dimensions of pooling matrices in PLs.\n",
    "\n",
    "\n",
    "NB : In an ideal world the model \"architecture\" could also be tuned with grid search, together with the hyperparameters listed above. To avoid exploding the parameters space, grid search was however performed by focusing only on the number and dimensions of feature maps in convolutional layers, and the dimensions of the pooling matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a unit_test function to extract cross-validated score for model architecture selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unit_test(classifier, nb_iter=3):\n",
    "    test_size = 0.2\n",
    "    random_state = 15\n",
    "    cv = StratifiedShuffleSplit(encoded_y, nb_iter,test_size=test_size,random_state=random_state)\n",
    "    clf = classifier()\n",
    "    scores = cross_val_score(clf, X=X_flat, y=encoded_y, scoring='accuracy', cv=cv)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a hyperparameter_optim function to perform grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyperparameter_optim(classifier, params, cv=3):\n",
    "\n",
    "    clf = GridSearchCV(classifier(), params, cv=cv, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "    print()\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining best model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Basic\" model with only one convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building corresponding classifier inheriting from sklearn.BaseEstimator\n",
    "##### Default hyperparameters\n",
    "- nb_filters = 32, filter_size = (3,3) in CL\n",
    "- pool_size = (2,2) in PL\n",
    "- nb_epochs = 10\n",
    "\n",
    "##### Early stopping\n",
    "- An early stopping condition based on the monitoring of the validation set accuracy is used so as to avoid overfitting and improve a bit the training time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters=32, filter_size=3, pool_size=2):\n",
    "        self.nb_filters = nb_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.pool_size = pool_size\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],64,64,3))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters = self.nb_filters,\n",
    "        filter_size = self.filter_size,\n",
    "        pool_size = self.pool_size \n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=10, verbose=1, callbacks=[earlyStopping], validation_split=0.1, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One CL combined to one PL (average pooling / no rectification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters'], hp['filter_size'], hp['filter_size'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(AveragePooling2D(pool_size=(hp['pool_size'],hp['pool_size'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One CL combined to one PL (max pooling / no rectification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters'], hp['filter_size'], hp['filter_size'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size'],hp['pool_size'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we know that max pooling performs better than average pooling, as suggested in [Jarrett et al. (2009)](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf).\n",
    "\n",
    "This very basic first model is composed of one CL followed by one PL(max) for feature extraction, and a single fully-connected layer with softmax activation for the classification step. The cross-validated accuracy obtained with this model is $\\sim$ 36%.\n",
    "\n",
    "In the following, max pooling will systematically be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One CL combined to one PL (with sigmoid non-linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters'], hp['filter_size'], hp['filter_size'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"sigmoid\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size'],hp['pool_size'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One CL combined to one PL (with relu non-linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters'], hp['filter_size'], hp['filter_size'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size'],hp['pool_size'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we know that adding sigmoid non-linearities deteriorate the performances, whereas relu rectification improves the classification accuracy.\n",
    "\n",
    "In what follows, relu activations will be used as rectification layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with two convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building corresponding classifier inheriting from sklearn.BaseEstimator\n",
    "##### Default hyperparameters\n",
    "- nb_filters_1 = 32, filter_size_1 = (3,3) in 1st CL\n",
    "- pool_size_1 = (2,2) in 1st PL\n",
    "- nb_filters_2 = 32, filter_size_2 = (3,3) in 2nd CL\n",
    "- pool_size_2 = (2,2) in 2nd PL\n",
    "- nb_epochs = 10\n",
    "\n",
    "##### Early stopping\n",
    "- An early stopping condition based on the monitoring of the validation set accuracy is used so as to avoid overfitting and improve a bit the training time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=32, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=32, filter_size_2=3, pool_size_2=2):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],64,64,3))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=10, verbose=1, callbacks=[earlyStopping], validation_split=0.1, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two CLs/PLs (no rectification layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two CLs/PLs (with relu layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture choice\n",
    "- From the results of the tests listed above, we retain as best model architecture for feature extraction : CL/relu/PL(max)/CL/relu/PL(max).\n",
    "- With this architecture, grid search will be performed to tune the number of filters in the CLs as well as their sizes, and the sizes of the pooling matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using grid search to tune the number of filters, and the size of the filters / pooling matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=32, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=32, filter_size_2=3, pool_size_2=2):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],64,64,3))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=2, callbacks=[earlyStopping], validation_split=0.1, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        time.sleep(0.1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "    \n",
    "\n",
    "\n",
    "params = {\n",
    "    'nb_filters_1': [32,64],\n",
    "    'filter_size_1': [3,6],\n",
    "    'pool_size_1': [2,4],\n",
    "    'nb_filters_2': [32,64],\n",
    "    'filter_size_2': [3,6],\n",
    "    'pool_size_2': [2,4]\n",
    "}\n",
    "clf = hyperparameter_optim(Classifier,params)\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the best model's performances on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters set found:\")\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "    \n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average classification accuracy is 48% on the test set. As regards the most represented class, the precision goes up to 70%. Precision can drop to 0% for certain classes, but this actually concerns classes that are under-represented in the dataset. This issue might be adressed by artificially increasing the number of instances for those under-represented classes to get a dataset that would be more balanced. This can be done by applying some transformations to available images, such as translations, rotations or changing luminosity, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a nutshell : Best model architecture and  hyperparameters\n",
    "\n",
    "The retained model architecture consists of two successive steps of feature extraction, each one being composed of :\n",
    "- a convolutional layer \n",
    "- a rectification layer with relu activation\n",
    "- a pooling layer (max pooling)\n",
    "\n",
    "The hyperparameters that gave the best performances are listed below : \n",
    "- nb_filters_1 = 64\n",
    "- filter_size_1 = 3\n",
    "- pool_size_1 = 2\n",
    "- nb_filters_2 = 64\n",
    "- filter_size_2 = 6\n",
    "- pool_size_2 = 4\n",
    "\n",
    "Interestingly, the filter and pooling matrices sizes are greater at the second stage, which means that the process of feature extraction passes consecutively through a \"fine\" step followed by a \"coarse\" step.\n",
    "\n",
    "In a way, pooling corresponds to \"losing\" information, that's why intuitively the contrary (going from \"coarse\" to \"fine\" feature extraction) might be useless : a refined step would be pointless after having thrown away some information !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the classification step\n",
    "\n",
    "Until now, we mainly focused on the feature extraction step to enhance the model's performances. In the above, the classification step simply consists in one fully-connected NN layer with softmax activation, which corresponds to a linear separation with respect to the output of the convolutional layers.\n",
    "\n",
    "In the following, we propose to refine the classification step. To do so, we use the \"best model\" described above to extract features and then plug them into a more elaborated classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=64, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=64, filter_size_2=6, pool_size_2=4):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],64,64,3))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=1, callbacks=[earlyStopping], validation_split=0.1, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        time.sleep(0.1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one hidden layer + relu non-linearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=200))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "    \n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need more hidden layers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=200))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=200))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    " \n",
    "print(unit_test(Classifier,nb_iter=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracies aren't improved by the additionnal hidden layer. In the following, we stick to a classification step that includes only one hidden layer. We propose to use grid search to tune its number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of units in the fully-connected hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(BaseEstimator):  \n",
    "\n",
    "    def __init__(self, nb_filters_1=64, filter_size_1=3, pool_size_1=2,\n",
    "                 nb_filters_2=64, filter_size_2=6, pool_size_2=4, nb_hunits=200):\n",
    "        self.nb_filters_1 = nb_filters_1\n",
    "        self.filter_size_1 = filter_size_1\n",
    "        self.pool_size_1 = pool_size_1\n",
    "        self.nb_filters_2 = nb_filters_2\n",
    "        self.filter_size_2 = filter_size_2\n",
    "        self.pool_size_2 = pool_size_2\n",
    "        self.nb_hunits = nb_hunits\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        X = X.reshape((X.shape[0],64,64,3))\n",
    "        X = (X / 255.)\n",
    "        X = X.astype(np.float32)\n",
    "        return X\n",
    "    \n",
    "    def preprocess_y(self, y):\n",
    "        return np_utils.to_categorical(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.preprocess(X)\n",
    "        y = self.preprocess_y(y)\n",
    "        \n",
    "        hyper_parameters = dict(\n",
    "        nb_filters_1 = self.nb_filters_1,\n",
    "        filter_size_1 = self.filter_size_1,\n",
    "        pool_size_1 = self.pool_size_1,\n",
    "        nb_filters_2 = self.nb_filters_2,\n",
    "        filter_size_2 = self.filter_size_2,\n",
    "        pool_size_2 = self.pool_size_2,\n",
    "        nb_hunits = self.nb_hunits\n",
    "        )\n",
    "        \n",
    "        print(\"FIT PARAMS : \")\n",
    "        print(hyper_parameters)\n",
    "        \n",
    "        self.model = build_model(hyper_parameters)\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=1, verbose=1, mode='auto')\n",
    "        self.model.fit(X, y, nb_epoch=20, verbose=1, callbacks=[earlyStopping], validation_split=0.1, \n",
    "                       validation_data=None, shuffle=True)\n",
    "        time.sleep(0.1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"PREDICT\")\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict_classes(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        print(\"SCORE\")\n",
    "        print(self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None))\n",
    "        return self.model.evaluate(self, X, y, batch_size=32, verbose=1, sample_weight=None) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    net = Sequential()\n",
    "    net.add(Convolution2D(hp['nb_filters_1'], hp['filter_size_1'], hp['filter_size_1'], border_mode='same', \n",
    "                          input_shape=(64,64,3)))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_1'],hp['pool_size_1'])))\n",
    "    net.add(Convolution2D(hp['nb_filters_2'], hp['filter_size_2'], hp['filter_size_2'], border_mode='same'))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(MaxPooling2D(pool_size=(hp['pool_size_2'],hp['pool_size_2'])))\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(output_dim=hp['nb_hunits']))\n",
    "    net.add(Activation(\"relu\"))\n",
    "    net.add(Dense(output_dim=18))\n",
    "    net.add(Activation(\"softmax\"))\n",
    "    \n",
    "    net.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "params = {\n",
    "    'nb_hunits': [100,200,300,400,500]\n",
    "}\n",
    "clf = hyperparameter_optim(Classifier,params)\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average model's accuracy has now increased up to 55%. Depending on the classes, precision and recall are quite heterogeneous : thay span from 0% to almost 100%.\n",
    "\n",
    "To answer the question \"are we satisfied with such performances ?\", well, it depends on *what* aim we want to achieve here.\n",
    "\n",
    "We can't say that this model is the best suited to classify accurately all pictures, independetely of the class they belong to. In particular, the model is very bad for under-represented classes. On the contrary, if the model is meant to detect in an efficient way all the instances of class 0, we can say that it does the job, as recall for this class is 98%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "nav_menu": {
    "height": "0px",
    "width": "0px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}